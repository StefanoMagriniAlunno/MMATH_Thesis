{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# genero i dati:\n",
        "N = 6\n",
        "n_points = [20, 20, 20, 30, 30, 40]\n",
        "noise_w = 0.3\n",
        "# scelgo casualmente in [-10,10]x[-10,10] N centroidi a valori reali\n",
        "centroids = np.random.rand(N, 2) * 20 - 10\n",
        "# stabilisco delle varianze casuali per questi centroidi tra [0.5, 2]\n",
        "variances = np.random.rand(N) * 1.5 + 0.5\n",
        "# stabilisco il numero di punti per ogni cluster\n",
        "# genero i punti\n",
        "data = [np.random.randn(n, 2) * variances[i] + centroids[i] for i, n in enumerate(n_points)]\n",
        "\n",
        "# inserisco del rumore: aggiungo dei punti uniformemente distribuiti in [-15,15]*[-15,15]\n",
        "noise = np.random.rand(int(noise_w*sum(n_points)), 2) * 30 - 15\n",
        "\n",
        "# plotto tutto\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "for i in range(N):\n",
        "    ax.scatter(data[i][:, 0], data[i][:, 1], label='Cluster %d' % i)\n",
        "\n",
        "ax.scatter(noise[:, 0], noise[:, 1], label='Noise', c='black', alpha=0.5)\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KMeans algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# eseguo il classico KMeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "fused_data = np.concatenate(data + [noise])\n",
        "kmeans = KMeans(n_clusters=N, init='k-means++', max_iter=1000)\n",
        "kmeans.fit(fused_data)\n",
        "labels = kmeans.predict(fused_data)\n",
        "\n",
        "# plotto i risultati:\n",
        "# i centroidi sono indicati con una x e i dati con dei cerchi\n",
        "# il colore di un cluster \u00e8 dettato dal label stimato con kmeans\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(N):\n",
        "    plt.scatter(fused_data[labels == i][:, 0], fused_data[labels == i][:, 1], label='Cluster %d' % i)\n",
        "    plt.scatter(kmeans.cluster_centers_[i][0], kmeans.cluster_centers_[i][1], marker='x', s=100, c='black')\n",
        "\n",
        "# la legenda la voglio fuori dal plot\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gaussian Mixture Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# eseguo il Gaussian Mixture Model\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from matplotlib.patches import Ellipse\n",
        "\n",
        "n_clusters = 6\n",
        "gmm = GaussianMixture(n_components=n_clusters, covariance_type='full', init_params='kmeans', max_iter=1000, tol=1e-6)\n",
        "\n",
        "gmm.fit(fused_data)\n",
        "labels = gmm.predict(fused_data)\n",
        "\n",
        "# plotto i risultati:\n",
        "# i centroidi sono indicati con una x e i dati con dei cerchi\n",
        "# il colore di un cluster \u00e8 dettato dal label stimato con gmm\n",
        "# indico con un'ellisse la covarianza del cluster\n",
        "# indico il peso del cluster con un numero vicino al centroide\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(N):\n",
        "    plt.scatter(fused_data[labels == i][:, 0], fused_data[labels == i][:, 1], label='Cluster %d' % i)\n",
        "    plt.scatter(gmm.means_[i][0], gmm.means_[i][1], marker='x', s=100, c='black')\n",
        "    # inserisco l'ellisse\n",
        "    cov = gmm.covariances_[i]\n",
        "    v, w = np.linalg.eigh(cov)\n",
        "    angle = np.arctan2(w[0][1], w[0][0])\n",
        "    angle = 180 * angle / np.pi\n",
        "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
        "    ell = Ellipse(xy=gmm.means_[i], width=v[0], height=v[1], angle=180 + angle, color='black', alpha=0.5)\n",
        "    plt.gca().add_patch(ell)\n",
        "    # peso del cluster\n",
        "    plt.text(gmm.means_[i][0], gmm.means_[i][1], '%.2f' % gmm.weights_[i], fontsize=12)\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FCM algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# eseguo la clusterizzazione FCM usando torch\n",
        "\n",
        "# importo da matplotlib le linee\n",
        "from matplotlib.lines import Line2D\n",
        "import torch\n",
        "# setto il device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# con kmeans stabilisco i centroidi iniziali\n",
        "kmeans = KMeans(n_clusters=N, init='k-means++', max_iter=1000)\n",
        "kmeans.fit(fused_data)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# calcolo la matrice delle distanze a 2 a 2 tra dati e centroidi\n",
        "data_torch = torch.tensor(fused_data, device=device)\n",
        "centroids = torch.tensor(centroids, device=device)\n",
        "m = 2\n",
        "\n",
        "for i in range(10000):\n",
        "    distances = torch.cdist(data_torch, centroids) ** (2/(m-1))\n",
        "    # calcolo la matrice di membership\n",
        "    U = 1 / (distances * torch.sum(1 / distances, dim=1, keepdim=True))\n",
        "\n",
        "    # calcolo i nuovi centroidi\n",
        "    Um = U ** m\n",
        "    centroids = torch.matmul(Um.T, data_torch) / torch.sum(Um, dim=0, keepdim=True).T\n",
        "\n",
        "# plotto i risultati:\n",
        "# i centroidi sono indicati con una x e i dat9i con dei cerchi\n",
        "# il colore di un cluster \u00e8 dettato dal label stimato con fcm (argmax)\n",
        "# il valore di tale collegamente \u00e8 indicato con una linea tanto pi\u00f9 chiara quanto pi\u00f9 \u00e8 basso\n",
        "plt.figure(figsize=(10, 10))\n",
        "labels = torch.argmax(U, dim=1)\n",
        "\n",
        "# passo a numpy per poter usare le funzioni di matplotlib\n",
        "labels = labels.cpu().numpy()\n",
        "centroids = centroids.cpu().numpy()\n",
        "U = U.cpu().numpy()\n",
        "for i in range(N):\n",
        "    plt.scatter(fused_data[labels==i][:,0], fused_data[labels==i][:,1], label='Cluster %d' % i)\n",
        "    plt.scatter(centroids[i,0], centroids[i,1], marker='x', s=100, c='black')\n",
        "for j in range(len(fused_data)):\n",
        "    middle = 1./len(centroids)\n",
        "    for i in range(N):\n",
        "        if U[j,i] >= middle:\n",
        "            line = Line2D(\n",
        "                [fused_data[j,0], centroids[i,0]],\n",
        "                [fused_data[j,1], centroids[i,1]],\n",
        "                alpha=U[j,i],\n",
        "                color='black',\n",
        "            )\n",
        "            plt.gca().add_line(line)\n",
        "    \n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.show()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cleaning and gray scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# leggo un'immagine non preprocessata\n",
        "img = Image.open(\"data/db/cutted_set/Author2/Author2_0001_01.png\").convert(\"L\")\n",
        "\n",
        "# mostro l'immagine a schermo\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Studio la densit\u00e0 dei grigi nell'immagine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# la converto in un array numpy\n",
        "img_array = np.array(img, dtype=np.float32)/256.0\n",
        "\n",
        "# mostro un grafico con la densit\u00e0 in scala di grigi\n",
        "# sull'ascisse un valore tra 0 e 1, sull'ordinata il numero di pixel con quel valore\n",
        "plt.hist(img_array.flatten(), bins=256, color='black', density=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# taglio i colori in base al loro valore rispetto un threshold\n",
        "threshold = 0.25\n",
        "img_array_cut = np.zeros_like(img_array)\n",
        "img_array_cut[img_array < threshold] = 0\n",
        "img_array_cut[img_array >= threshold] = 1\n",
        "\n",
        "# mostro l'immagine a schermo\n",
        "plt.imshow(img_array_cut, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# eseguo una clusterizzazione kmeans su tutti i valori tra 0 e 1 con n_centroids\n",
        "n_centroids = 8\n",
        "img_flat = img_array.reshape(-1, 1)\n",
        "kmeans = KMeans(n_clusters=n_centroids, init='k-means++', max_iter=1000)\n",
        "kmeans.fit(img_flat)\n",
        "labels = kmeans.predict(img_flat)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# controllo che il numero di iterazioni non abbia superato max_iter\n",
        "if kmeans.n_iter_ == kmeans.max_iter:\n",
        "    print('KMeans non ha convergenza')\n",
        "\n",
        "# riodino i centroidi e i labels in ordine crescente\n",
        "order = np.argsort(centroids.flatten())\n",
        "centroids = centroids[order]\n",
        "new_labels = np.zeros_like(labels)\n",
        "for i, o in enumerate(order):\n",
        "    new_labels[labels == o] = i\n",
        "labels = new_labels\n",
        "\n",
        "# Normalize labels to range [0, 1]\n",
        "normalized_labels = labels / labels.max()\n",
        "\n",
        "# Create a colormap\n",
        "cmap = plt.get_cmap('viridis')\n",
        "\n",
        "# Plot histogram for each cluster\n",
        "hist, bin_edges = np.histogram(img_array.flatten(), bins=256, density=False)\n",
        "for i in range(n_centroids):\n",
        "    cluster_data = img_flat[labels == i]\n",
        "    cluster_hist, _ = np.histogram(\n",
        "        cluster_data,\n",
        "        bins=bin_edges,\n",
        "        density=False\n",
        "    )\n",
        "    plt.hist(\n",
        "        bin_edges[:-1],\n",
        "        bins=bin_edges,\n",
        "        weights=cluster_hist / hist.sum() * 256,\n",
        "        color=cmap(i / n_centroids),\n",
        "        alpha=0.5,\n",
        "    )\n",
        "\n",
        "# Add vertical lines for centroids\n",
        "for c in centroids:\n",
        "    plt.axvline(c, color='blue')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ricostruisco l'immagine originale usando i centroidi\n",
        "img_reconstructed = centroids[labels].reshape(img_array.shape)\n",
        "\n",
        "# mostro l'immagine ricostruita\n",
        "plt.imshow(img_reconstructed, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nell'immagine faccio s\u00ec che i pixel del cluster k-esimo siano colorati di rosso\n",
        "\n",
        "# cluster scelto\n",
        "for k in range(n_centroids):\n",
        "    # creo un'immagine vuota\n",
        "    img_colored = np.zeros(img_array.shape + (3,))\n",
        "    # coloro i pixel del cluster k-esimo di rosso\n",
        "    img_colored_flat = img_colored.reshape(-1, 3)\n",
        "    img_colored_flat[labels == k] = [1, 0, 0]\n",
        "    img_colored = img_colored_flat.reshape(img_array.shape + (3,))\n",
        "\n",
        "    # riproduco l'immagine\n",
        "    plt.imshow(img_colored)\n",
        "    plt.title('Cluster %d' % k)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# normalizzo l'immagine affinch\u00e9 solo i claster da A a B siano visibili (con valori tra 0 e 1)\n",
        "\n",
        "# scritte in 14-20/64\n",
        "\n",
        "A = 0\n",
        "B = 5\n",
        "# cerco il colore del pixel dentro il cluster A di valore minimo\n",
        "min_A = img_flat[labels == A].min()\n",
        "max_B = img_flat[labels == B].max()\n",
        "img_normalized = (img_array - min_A) / (max_B - min_A)\n",
        "# taglio i valori < 0 a 0 e i valori > 1 a 1\n",
        "img_normalized[img_normalized < 0] = 0\n",
        "img_normalized[img_normalized > 1] = 1\n",
        "\n",
        "# salvo l'immagine come png\n",
        "img_normalized = Image.fromarray(np.uint8(img_normalized*255))\n",
        "img_normalized.save('data/out/normalized.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distanza tra distribuzioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_dimensions = 16\n",
        "n_clusters = 2\n",
        "n_latbox = 2\n",
        "n_data = 128\n",
        "n_test = 1000\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "results = []\n",
        "time_start = time.time()\n",
        "for _ in range(n_test):\n",
        "    A = torch.randn(n_data, n_dimensions, device=device)\n",
        "    B = torch.randn(n_data, n_dimensions, device=device) + torch.ones(n_dimensions, device=device) / (n_dimensions**0.5)\n",
        "    fused_d = torch.cat([A, B])\n",
        "    fused_d = fused_d.view(-1, fused_d.shape[-1])\n",
        "\n",
        "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=1000)\n",
        "    fit = kmeans.fit(fused_d.cpu().numpy())\n",
        "    centers = torch.from_numpy(fit.cluster_centers_).to(device=device)\n",
        "    distances = torch.cdist(fused_d, centers)\n",
        "    mu = torch.zeros(n_clusters, device=device)\n",
        "    for i in range(n_clusters):\n",
        "        mu[i] = torch.mean(distances[fit.labels_ == i, i])\n",
        "    mu = (mu**0.5)\n",
        "    p_A = torch.zeros(n_clusters, device=device)\n",
        "    p_B = torch.zeros(n_clusters, device=device)\n",
        "    for i in range(n_clusters):\n",
        "        p_A[i] = np.sum(fit.labels_[:A.shape[0]] == i) / A.shape[0]\n",
        "        p_B[i] = np.sum(fit.labels_[A.shape[0]:] == i) / B.shape[0]\n",
        "    s = 0\n",
        "    d = 0\n",
        "    for i in range(n_clusters):\n",
        "        s += (p_A[i] - p_B[i])**2 / (p_A[i] + p_B[i])**2 * mu[i]\n",
        "        d += mu[i]\n",
        "    s /= d\n",
        "    D_and=0\n",
        "    D_or=0\n",
        "    for i in range(n_clusters):\n",
        "        D_and += 1 if p_A[i] > 0 and p_B[i] > 0 else 0\n",
        "        D_or += 1 if p_B[i] > 0 or p_B[i] > 0 else 0\n",
        "    jaccard = (1 + D_and / D_or)**(-1)\n",
        "    results.append(jaccard * s.cpu())\n",
        "time_end = time.time()\n",
        "\n",
        "print(np.mean(results))\n",
        "print(np.var(results))\n",
        "print((time_end - time_start)/n_test)\n",
        "print()\n",
        "\n",
        "results = []\n",
        "time_start = time.time()\n",
        "for _ in range(n_test):\n",
        "    A = torch.randn(n_data, n_dimensions, device=device)\n",
        "    B = torch.randn(n_data, n_dimensions, device=device) + torch.ones(n_dimensions, device=device) / (n_dimensions**0.5)\n",
        "    fused_d = torch.cat([A, B])\n",
        "    fused_d = fused_d.view(-1, fused_d.shape[-1])\n",
        "\n",
        "    labels = torch.zeros(fused_d.shape[0], device='cuda', dtype=torch.int32)\n",
        "    for i in range(fused_d.shape[1]):\n",
        "        step = (torch.max(fused_d[:,i]) - torch.min(fused_d[:,i])) / n_latbox\n",
        "        m = torch.min(fused_d[:,i])\n",
        "        labels = labels*int(n_latbox) + torch.clamp(((fused_d[:,i] - m) // step).to(torch.int32), min=0, max=n_latbox - 1)\n",
        "    \n",
        "    # sort labels with counting sort\n",
        "    countA: dict = {}\n",
        "    for l in labels[:A.shape[0]]:\n",
        "        if l in [k[0] for k in countA.items()]:\n",
        "            countA[l.item()] += 1\n",
        "        else:\n",
        "            countA[l.item()] = 1\n",
        "    \n",
        "    countB: dict = {}\n",
        "    for l in labels[A.shape[0]:]:\n",
        "        if l in [k[0] for k in countB.items()]:\n",
        "            countB[l.item()] += 1\n",
        "        else:\n",
        "            countB[l.item()] = 1\n",
        "    \n",
        "    A_keys = {k[0] for k in countA.items()}\n",
        "    B_keys = {k[0] for k in countB.items()}\n",
        "\n",
        "    s = 0\n",
        "    all_keys = A_keys.union(B_keys)\n",
        "    for i in all_keys:\n",
        "        if i in A_keys:\n",
        "            if i in B_keys:\n",
        "                s += (countA[i]/A.shape[0] - countB[i]/B.shape[0])**2/(countA[i]/A.shape[0] + countB[i]/B.shape[0])**2\n",
        "            else:\n",
        "                s += 1\n",
        "        elif i in B_keys:\n",
        "            s += 1\n",
        "    s /= (len(A_keys) + len(B_keys))\n",
        "    results.append(s)\n",
        "time_end = time.time()\n",
        "\n",
        "print(np.mean(results))\n",
        "print(np.var(results))\n",
        "print((time_end - time_start)/n_test)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# eseguo una clusterizzazione kmeans sulla fusione dei dati considerando i pesi\n",
        "fused_d = torch.cat([A, B])\n",
        "fused_w = torch.cat([w_A, w_B])\n",
        "n_clusters = 64\n",
        "\n",
        "fused_d = fused_d.view(-1, fused_d.shape[-1])\n",
        "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=1000)\n",
        "\n",
        "fit = kmeans.fit(fused_d.cpu().numpy(), sample_weight=fused_w.cpu().numpy())\n",
        "\n",
        "centers = torch.from_numpy(fit.cluster_centers_).to(device=device)\n",
        "\n",
        "# plot dei centroidi (o neri) assieme alla distribuzione dei dati (scatter blu)\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=fused_d[:, 0].cpu().numpy(), \n",
        "    y=fused_d[:, 1].cpu().numpy(),\n",
        "    z=fused_d[:, 2].cpu().numpy(),\n",
        "    mode='markers', \n",
        "    marker=dict(size=2, opacity=0.8, color='blue'), \n",
        "    name='Data'\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=centers[:, 0].cpu().numpy(), \n",
        "    y=centers[:, 1].cpu().numpy(), \n",
        "    z=centers[:, 2].cpu().numpy(), \n",
        "    mode='markers', \n",
        "    marker=dict(size=3, color='black', symbol='x'), \n",
        "    name='Centroids'\n",
        "))\n",
        "fig.update_layout(width=800, height=800)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calcolo la distanza usando kmeans\n",
        "\n",
        "# calcolo i pesi dei cluster come le medie quadratiche delle distanze dei dati dai rispettivi centroidi\n",
        "distances = torch.cdist(fused_d, centers)\n",
        "mu = torch.zeros(n_clusters, device=device)\n",
        "for i in range(n_clusters):\n",
        "    # uso i label per selezionare solo le distanze del cluster i-esimo\n",
        "    mu[i] = torch.mean(distances[fit.labels_ == i, i])\n",
        "mu = (mu**0.5)\n",
        "# realizzo il grafico precedente mettendo per\u00f2 delle sfere di raggio w attorno ai centroidi\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=fused_d[:, 0].cpu().numpy(), \n",
        "    y=fused_d[:, 1].cpu().numpy(),\n",
        "    z=fused_d[:, 2].cpu().numpy(),\n",
        "    mode='markers', \n",
        "    marker=dict(size=2, opacity=0.8, color='blue'), \n",
        "    name='Data'\n",
        "))\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=centers[:, 0].cpu().numpy(), \n",
        "    y=centers[:, 1].cpu().numpy(), \n",
        "    z=centers[:, 2].cpu().numpy(), \n",
        "    mode='markers', \n",
        "    marker=dict(size=3, color='black', symbol='x'), \n",
        "    name='Centroids'\n",
        "))\n",
        "def make_sphere(center, radius):\n",
        "    d = torch.tensor(np.pi/32, device=device)\n",
        "\n",
        "    theta, phi = torch.meshgrid(torch.arange(0, torch.pi + d, d, device=device), torch.arange(0, 2 * torch.pi + d, d, device=device))\n",
        "    # Convert to Cartesian coordinates\n",
        "    x = torch.sin(theta) * torch.cos(phi)\n",
        "    y = torch.sin(theta) * torch.sin(phi)\n",
        "    z = torch.cos(theta)\n",
        "    points = torch.vstack([x.ravel(), y.ravel(), z.ravel()])\n",
        "    points = points * radius[:, None] + center[:, None]\n",
        "    x, y, z = points\n",
        "    return x,y,z\n",
        "\n",
        "# assegno un colore ad ogni centroide\n",
        "for i in range(n_clusters):\n",
        "    x, y, z = make_sphere(centers[i], torch.tensor([mu[i], mu[i], mu[i]], device='cuda'))\n",
        "    fig.add_trace(go.Mesh3d(x=x.cpu().numpy(), y=y.cpu().numpy(), z=z.cpu().numpy(), opacity=0.5, color='lightpink', alphahull=0))\n",
        "\n",
        "fig.update_layout(width=800, height=800)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calcolo il peso di ogni cluster rispettivamente per le due distribuzioni A e B\n",
        "p_A = torch.zeros(n_clusters, device=device)\n",
        "p_B = torch.zeros(n_clusters, device=device)\n",
        "for i in range(n_clusters):\n",
        "    p_A[i] = np.sum(fit.labels_[:A.shape[0]] == i) / A.shape[0]\n",
        "    p_B[i] = np.sum(fit.labels_[A.shape[0]:] == i) / B.shape[0]\n",
        "\n",
        "# calcolo la distanza tra le distribuzioni A e B:\n",
        "s = 0\n",
        "d = 0\n",
        "for i in range(n_clusters):\n",
        "    s += (p_A[i] - p_B[i])**2 / (p_A[i] + p_B[i])**2 * mu[i]\n",
        "    d += mu[i]\n",
        "s /= d\n",
        "\n",
        "# jaccard index\n",
        "# calcolo la quantit\u00e0 di centroidi che contengono elementi di A\n",
        "D_and=0\n",
        "D_or=0\n",
        "for i in range(n_clusters):\n",
        "    D_and += 1 if p_A[i] > 0 and p_B[i] > 0 else 0\n",
        "    D_or += 1 if p_B[i] > 0 or p_B[i] > 0 else 0\n",
        "\n",
        "jaccard = (1 + D_and / D_or)**(-1)\n",
        "\n",
        "print(s)\n",
        "print(jaccard)\n",
        "print(jaccard * s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lateral_section = 6\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "results = []\n",
        "\n",
        "for _ in range(1000):\n",
        "    A_1 = torch.randn(4, 3, device=device) + torch.tensor([-2, 0, 1], device=device)\n",
        "    A_2 = torch.randn(8, 3, device=device) * 2 + torch.tensor([4, 0, 0], device=device)\n",
        "    A_3 = torch.randn(20, 3, device=device) * 0.5 + torch.tensor([0, 4, 0], device=device)\n",
        "    A_noise = torch.rand(0, 3, device=device) * 20 - 10\n",
        "    # concateno i dati\n",
        "    A = torch.cat([A_1, A_2, A_3, A_noise], dim=0)\n",
        "    w_A = torch.ones(A.shape[0], device=device)/A.shape[0]\n",
        "\n",
        "    B_1 = torch.randn(12, 3, device=device) * torch.tensor([0.5, 2, -1], device=device) + torch.tensor([-1, 0, 0], device=device)\n",
        "    B_2 = torch.randn(12, 3, device=device) + torch.tensor([4, 0, 0], device=device)\n",
        "    B_3 = torch.randn(12, 3, device=device) * 0.5 + torch.tensor([0, -4, 0], device=device)\n",
        "    B_noise = torch.rand(0, 3, device=device) * 20 - 10\n",
        "    # concateno i dati\n",
        "    B = torch.cat([B_1, B_2, B_3, B_noise], dim=0)\n",
        "    w_B = torch.ones(B.shape[0], device=device)/B.shape[0]\n",
        "\n",
        "    fused_d = torch.cat([A, B])\n",
        "\n",
        "    # plotto i dati in 3d\n",
        "    \"\"\"fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=torch.cat([A[:, 0], B[:, 0]]).cpu().numpy(), \n",
        "        y=torch.cat([A[:, 1], B[:, 1]]).cpu().numpy(), \n",
        "        z=torch.cat([A[:, 2], B[:, 2]]).cpu().numpy(), \n",
        "        mode='markers', \n",
        "        marker=dict(\n",
        "            size=3, \n",
        "            opacity=1.0, \n",
        "            color=['blue']*A.shape[0] + ['red']*B.shape[0]\n",
        "        ), \n",
        "        name='A and B'\n",
        "    ))\n",
        "    # rendo l'immagine pi\u00f9 grande\n",
        "    fig.update_layout(width=800, height=800)\n",
        "    fig.show()\"\"\"\n",
        "\n",
        "    # calcolo la distanza usando per\u00f2 partizioni statiche\n",
        "    labels = torch.zeros(fused_d.shape[0], device='cuda')\n",
        "    for i in range(fused_d.shape[1]):\n",
        "        step = (torch.max(fused_d[:,i]) - torch.min(fused_d[:,i])) / lateral_section\n",
        "        m = torch.min(fused_d[:,i])\n",
        "        labels = labels*lateral_section + (fused_d[:,i] - m) // step\n",
        "    labels_A = labels[:A.shape[0]]\n",
        "    labels_B = labels[A.shape[0]:]\n",
        "    # calcolo la distanza nel modo classico\n",
        "    s = torch.tensor([0.0], device='cuda')\n",
        "    dA = torch.tensor([0.0], device='cuda')\n",
        "    dB = torch.tensor([0.0], device='cuda')\n",
        "    for i in range(lateral_section**3):\n",
        "        f_A = torch.count_nonzero(labels_A == i) / labels_A.shape[0]\n",
        "        f_B = torch.count_nonzero(labels_B == i) / labels_B.shape[0]\n",
        "        if f_A > 0:\n",
        "            dA += 1\n",
        "            if f_B > 0:\n",
        "                dB += 1\n",
        "                s += (f_A - f_B)**2 / (f_A + f_B)**2\n",
        "            else:\n",
        "                s += 1\n",
        "        elif f_B > 0:\n",
        "            dB += 1\n",
        "            s += 1\n",
        "    results.append((s/(dA+dB)).item())\n",
        "\n",
        "print(np.var(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# uso centroids come initial centroids per fcm\n",
        "iter = 10\n",
        "fcm_centers = centers.clone()\n",
        "for i in range(iter):\n",
        "    distances = torch.cdist(fused_d, fcm_centers) ** 2\n",
        "    \n",
        "    # eseguo operazioni distinte su ogni riga di distances:\n",
        "    # se la riga contiene uno 0 allora setto 1 dove ci sono 0 e 0 altrove\n",
        "    # altrimenti calcolo la membership\n",
        "    special_rows = torch.any(distances == 0, dim=1, keepdim=True)\n",
        "    min_values = torch.min(distances, dim=1)[0]\n",
        "    U = torch.where(special_rows, \n",
        "        distances != 0, # if row contains 0, set 1 where 0 and 0 elsewhere\n",
        "        min_values[:,None] / distances, # else calculate membership\n",
        "    )\n",
        "    U = (U / U.sum(dim=1, keepdim=True))**2\n",
        "    fcm_centers = torch.matmul(U.T, fused_d) / torch.sum(U, dim=0, keepdim=True).T\n",
        "\n",
        "centers = fcm_centers.clone()\n",
        "# plot dei centroidi (o neri) assieme alla distribuzione dei dati (scatter blu)\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.scatter(fused_d[:, 0].cpu(), fused_d[:, 1].cpu(), s=2, alpha=0.5, color='blue', label='Data')\n",
        "plt.scatter(centers[:, 0].cpu(), centers[:, 1].cpu(), s=4, color='black', marker='o', label='Centroids')\n",
        "# aggiungo le linee di membership, se la membership \u00e8 superiore a 1/n_clusters inserisco una linea\n",
        "indices = torch.where(U >= 1./n_clusters)\n",
        "for i in range(len(indices[0])):\n",
        "    plt.plot(\n",
        "        [fused_d[indices[0][i], 0].cpu(), centers[indices[1][i], 0].cpu()],\n",
        "        [fused_d[indices[0][i], 1].cpu(), centers[indices[1][i], 1].cpu()],\n",
        "        color='black', alpha=U[indices[0][i], indices[1][i]].cpu().item()\n",
        "    )\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.patches import Circle\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "# se si usa kmeans U[i,j] = 1 se il punto i \u00e8 nel cluster j altrimenti 0\n",
        "labels = kmeans.predict(fused_d.cpu().numpy())\n",
        "U = torch.zeros(fused_d.shape[0], n_clusters, device=device)\n",
        "for i in range(fused_d.shape[0]):\n",
        "    U[i, labels[i]] = 1\n",
        "\n",
        "# definisco la misura dei cluster\n",
        "distances = torch.cdist(fused_d, centers) ** 2\n",
        "mu = (torch.einsum('ij,ij->j', U, distances) / torch.sum(U, dim=0))**(fused_d.shape[1]/2)\n",
        "# calcolo il peso di A e B sui cluster\n",
        "omega_A = torch.mean(U[:A.shape[0], :], dim=0)  # A \u00e8 il primo cluster\n",
        "omega_B = torch.mean(U[A.shape[0]:, :], dim=0)  # B \u00e8 il secondo cluster\n",
        "# calcolo i rapporti tra i pesi su ogni cluster\n",
        "r = torch.min(omega_A, omega_B) / torch.max(omega_A, omega_B)\n",
        "\n",
        "# plotto lo spazio di misura usato usando mu(c)**(1/fused_d.shape[1]) come raggio del cluster c\n",
        "\n",
        "# colormap\n",
        "colors = [(1, 0, 0), (0, 0, 1), (0, 1, 0)]\n",
        "n_bins = 256  # numero di intervalli nella colormap\n",
        "cmap_name = 'membership_colormap'\n",
        "my_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "ax.scatter(fused_d[:, 0].cpu(), fused_d[:, 1].cpu(), s=4, alpha=0.4, color='blue')\n",
        "for i in range(n_clusters):\n",
        "    s = (omega_B[i] - omega_A[i])/(omega_B[i] + omega_A[i])\n",
        "    circle = Circle(\n",
        "        centers[i].cpu(),\n",
        "        mu[i].cpu()**(1/fused_d.shape[1]),\n",
        "        color=my_cmap(s.cpu()),\n",
        "        alpha=0.8)\n",
        "    ax.add_patch(circle)\n",
        "\n",
        "# inserisco la mappa di colori\n",
        "# Crea manualmente la colorbar\n",
        "norm = plt.Normalize(vmin=0, vmax=1)\n",
        "sm = cm.ScalarMappable(cmap=my_cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "cbar = fig.colorbar(sm, ax=ax)  # Associa la colorbar all'asse 'ax'\n",
        "cbar.set_label('membership')\n",
        "cbar.set_ticks([0, 1])  # Imposta i tick manualmente\n",
        "cbar.set_ticklabels(['A', 'B'])  # Imposta le etichette dei tick\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Indice di Jaccard\n",
        "# computo D_A come la membership dei centroidi in A e D_B come la membership dei centroidi in B\n",
        "D_A = torch.max(U[:A.shape[0], :], dim=0)[0]\n",
        "D_B = torch.max(U[A.shape[0]:, :], dim=0)[0]\n",
        "# computo la cardinalit\u00e0 dell'unione e dell'intersezione\n",
        "card_inter = torch.sum(torch.min(D_A, D_B))\n",
        "card_union = torch.sum(torch.max(D_A, D_B))\n",
        "# computo l'indice di Jaccard\n",
        "J = card_inter / card_union\n",
        "print(\"Cardinalit\u00e0 di D_A: \", torch.sum(D_A))\n",
        "print(\"Cardinalit\u00e0 di D_B: \", torch.sum(D_B))\n",
        "print(\"Cardinalit\u00e0 dell'intersezione: \", card_inter)\n",
        "print(\"Cardinalit\u00e0 dell'unione: \", card_union)\n",
        "print(\"Indice di Jaccard: \", J)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calcoliamo la distanza tra A e B\n",
        "integral = (((r - 1)/(r + 1))**2 @ mu)/(torch.sum(mu))\n",
        "reguliser = (1+J)**(-1)\n",
        "distance = integral * reguliser\n",
        "\n",
        "print(\"Integrale: \", integral)\n",
        "print(\"Regolarizzatore: \", reguliser)\n",
        "print(\"Distanza tra A e B: \", distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plotto lo spazio di misura ottenuto:\n",
        "# evidenzio i cluster con un grafico a barre\n",
        "# ogni barra ha un'altezza proporzionale al peso del cluster e un ampiezza proporzionale alla misura\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# aggiungo gli scatter fused_d in blu\n",
        "plt.scatter(fused_d[:, 0].cpu(), fused_d[:, 1].cpu(), color='blue', alpha=0.5, s=2)\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    # creo una sfera con raggio proporzionale alla misura\n",
        "    # il suo colore dipende da weights ed \u00e8 indicato con color\n",
        "    circle = Circle(\n",
        "        centers[i].cpu().numpy(),\n",
        "        measure[i].item()**(1/fused_d.shape[1]),\n",
        "        color=(1-weights[i].item(), 0, weights[i].item()),\n",
        "        alpha=0.8,\n",
        "    )\n",
        "    # aggiungo il rettangolo al grafico\n",
        "    plt.gca().add_patch(circle)\n",
        "\n",
        "\n",
        "plt.xlim(-10, 10)\n",
        "plt.ylim(-10, 10)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fit di A e B su siffatta clusterizzazione\n",
        "labels_A = fit.predict(A.cpu().numpy().reshape(-1, fused_d.shape[1]))\n",
        "labels_B = fit.predict(B.cpu().numpy().reshape(-1, fused_d.shape[1]))\n",
        "\n",
        "# densit\u00e0 di probabilit\u00e0 di A e B sui cluster\n",
        "p_A = torch.zeros(n_clusters, device=device)\n",
        "p_B = torch.zeros(n_clusters, device=device)\n",
        "for i in range(n_clusters):\n",
        "    p_A[i] = torch.sum(w_A[labels_A == i])\n",
        "    p_B[i] = torch.sum(w_B[labels_B == i])\n",
        "p_A /= torch.sum(p_A)\n",
        "p_B /= torch.sum(p_B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plotto le densit\u00e0\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# colormap\n",
        "colors = [(1, 0, 0), (0.5, 0.5, 1), (0, 1, 0)]\n",
        "n_bins = 256  # numero di intervalli nella colormap\n",
        "cmap_name = 'membership_colormap'\n",
        "my_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
        "\n",
        "fig, ax = plt.subplots()  # Crea una figura e un asse\n",
        "for i in range(n_clusters):\n",
        "    if measure[i] == 0:\n",
        "        # metto una X blu per i cluster vuoti\n",
        "        ax.scatter(centers[i,0].item(), centers[i,1].item(), marker='x', color='blue')\n",
        "    else:\n",
        "        # calcolo l'addendo senza elevare al quadrato\n",
        "        s = ((p_A[i] - p_B[i])/(p_A[i] + p_B[i]))\n",
        "        s = (s + 1)/2\n",
        "        # creo un cerchio per il cluster i-esimo\n",
        "        circ = Circle(\n",
        "            centers[i].cpu(),\n",
        "            measure[i].item()**(1/fused_d.shape[1]),\n",
        "            color=my_cmap(s.cpu()),\n",
        "            alpha=1.0,\n",
        "        )\n",
        "        # aggiungo il rettangolo al grafico\n",
        "        ax.add_patch(circ)\n",
        "\n",
        "# inserisco la mappa di colori\n",
        "# Crea manualmente la colorbar\n",
        "norm = plt.Normalize(vmin=0, vmax=1)\n",
        "sm = cm.ScalarMappable(cmap=my_cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "cbar = fig.colorbar(sm, ax=ax)  # Associa la colorbar all'asse 'ax'\n",
        "cbar.set_label('membership')\n",
        "\n",
        "plt.xlim(-6, 12)\n",
        "plt.ylim(-9, 8)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ne calcolo la distanza\n",
        "D_A = p_A > 0\n",
        "D_B = p_B > 0\n",
        "\n",
        "integral = torch.where(D_A | D_B, measure * ((p_A-p_B)/(p_A+p_B))**2, torch.zeros(n_clusters, device=device)).sum() / measure[D_A | D_B].sum()\n",
        "Jaccard_index = measure[D_A & D_B].sum() / measure[D_A | D_B].sum()\n",
        "\n",
        "distance = (1 + Jaccard_index)**(-1) * integral\n",
        "print(distance.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## controprova senza clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nodes_perdim = 32\n",
        "\n",
        "# centri dei cluster sono una meshgrid\n",
        "centers = torch.meshgrid([torch.linspace(-6, 12, nodes_perdim), torch.linspace(-9, 8, nodes_perdim)])\n",
        "x_radius = 9 / nodes_perdim\n",
        "y_radius = 8.5 / nodes_perdim\n",
        "\n",
        "# la misura \u00e8 unica e uguale per tutti quindi non serve\n",
        "# calcolo le densit\u00e0 di probabilit\u00e0 di A e B sui cluster\n",
        "p_A = torch.zeros((nodes_perdim,nodes_perdim), device=device)\n",
        "p_B = torch.zeros((nodes_perdim,nodes_perdim), device=device)\n",
        "for i in range(nodes_perdim):\n",
        "    for j in range(nodes_perdim):\n",
        "        # conto quanti punti di A e B sono nel cluster i-esimo, ossia nel range [centers[i]-radius, centers[i]+radius]\n",
        "        p_A[i,j] = torch.sum(\n",
        "            (A[:,0] >= centers[0][i,j]-x_radius) & (A[:,0] < centers[0][i,j]+x_radius) &\n",
        "            (A[:,1] >= centers[1][i,j]-y_radius) & (A[:,1] < centers[1][i,j]+y_radius)\n",
        "        )\n",
        "        p_B[i,j] = torch.sum(\n",
        "            (B[:,0] >= centers[0][i,j]-x_radius) & (B[:,0] < centers[0][i,j]+x_radius) &\n",
        "            (B[:,1] >= centers[1][i,j]-y_radius) & (B[:,1] < centers[1][i,j]+y_radius)\n",
        "        )\n",
        "p_A /= torch.sum(p_A)\n",
        "p_B /= torch.sum(p_B)\n",
        "\n",
        "p_A = p_A.view(-1)\n",
        "p_B = p_B.view(-1)\n",
        "\n",
        "# calcolo le distanze\n",
        "D_A = p_A > 0\n",
        "D_B = p_B > 0\n",
        "integral = torch.where(D_A | D_B, ((p_A-p_B)/(p_A+p_B))**2, torch.zeros(nodes_perdim*nodes_perdim, device=device)).sum() / torch.count_nonzero(D_A | D_B)\n",
        "Jaccard_index = torch.count_nonzero(D_A & D_B) / torch.count_nonzero(D_A | D_B)\n",
        "\n",
        "distance = (1 + Jaccard_index)**(-1) * integral\n",
        "print(distance.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FFT 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "def my_map(X, Y):\n",
        "    return 2*torch.cos(2*math.pi*(2*X + 3*Y) + 3) + 0.8*torch.cos(2*math.pi*(X + 5*Y) + 2) + torch.cos(2*math.pi*(7*X + 5*Y)) + torch.randn(X.shape)\n",
        "\n",
        "# genero X,Y in una matrice 32x16 con valori da 0\n",
        "X, Y = torch.meshgrid([torch.linspace(0, 1, 64), torch.linspace(0, 1, 32)])\n",
        "\n",
        "Z = my_map(X,Y)\n",
        "\n",
        "# calcolo la trasformata di fourier 2d con fft\n",
        "Z_fft = torch.fft.fft2(Z)\n",
        "\n",
        "# plotting\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# creo 2 plot disposti orizzontalmente\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
        "\n",
        "# in ax[0] mostro la mappa Z\n",
        "ax[0].imshow(Z.cpu().numpy(), cmap='viridis')\n",
        "\n",
        "# in ax[1] mostro la mappa Z_fft\n",
        "ax[1].imshow(torch.abs(Z_fft).cpu().numpy(), cmap='viridis')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L = [{0}]\n",
        "F = [{1}]\n",
        "\n",
        "index = 0\n",
        "for i in range(1,10000):\n",
        "    for j in range(len(L)-1, -1, -1):\n",
        "        # controllo se L[j-1] si interseca con F[index] se non si interseca siamo a cavallo\n",
        "        if not (L[j] & F[index]) and not i in F[j]:\n",
        "            # i due insiemi non si intersecano\n",
        "            L[j].add(i)\n",
        "            F[j].add(i+1)\n",
        "            F[j].add(i-1)\n",
        "            index = j\n",
        "            break\n",
        "    else:\n",
        "        # non esiste alcun j valido quindi incremento\n",
        "        L.append({i})\n",
        "        F.append({i+1, i-1})\n",
        "        index = -1\n",
        "\n",
        "# trasformo in lista ordinata ogni insieme in L\n",
        "for i in range(len(L)):\n",
        "    L[i] = sorted(list(L[i]))\n",
        "\n",
        "for l in L:\n",
        "    print(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prendo due immagini e le preprocesso\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from source.packages import cleaner\n",
        "import logging\n",
        "\n",
        "cleaner.fft(\n",
        "    logging.getLogger(),    \n",
        "    \"data/db/cutted_set/Author3\",\n",
        "    \"data/.out\",\n",
        "    False,\n",
        "    0.001,\n",
        "    'cuda'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# open image\n",
        "img = Image.open(\"data/db/cutted_set/Author3/Author3_0001_01.png\").convert(\"L\")\n",
        "# plot dell'immagine\n",
        "plt.subplot(2,2,1)\n",
        "plt.title(\"my image\", cmap='gray')\n",
        "plt.imshow(img)\n",
        "plt.subplot(2,2,2)\n",
        "plt.title(\"my image\")\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare Stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-22 22:34:14,293 - root - INFO - fuzzy parameters: 3.612734079360962 shared centroids, 3.702620029449463 total centroids\n",
            "2024-12-22 22:34:14,294 - root - INFO - mean integral: 0.014932204969227314\n",
            "2024-12-22 22:34:14,294 - root - INFO - distance: 0.0075578405521810055\n",
            "2024-12-22 22:34:14,295 - root - INFO - Computed distance between ./data/.out/synthetized/Author1/Author1_0040_09.png, ./data/.out/synthetized/Author3/Author3_0008_03.png: 0.0075578405521810055\n",
            "2024-12-22 22:34:17,710 - root - INFO - fuzzy parameters: 4.352913856506348 shared centroids, 4.465834617614746 total centroids\n",
            "2024-12-22 22:34:17,711 - root - INFO - mean integral: 0.014846491627395153\n",
            "2024-12-22 22:34:17,712 - root - INFO - distance: 0.007518297526985407\n",
            "2024-12-22 22:34:17,712 - root - INFO - Computed distance between ./data/.out/synthetized/Author1/Author1_0040_09.png, ./data/.out/synthetized/Author3/Author3_0008_03.png: 0.007518297526985407\n",
            "2024-12-22 22:34:22,351 - root - INFO - fuzzy parameters: 3.6288905143737793 shared centroids, 3.6995718479156494 total centroids\n",
            "2024-12-22 22:34:22,351 - root - INFO - mean integral: 0.014906732365489006\n",
            "2024-12-22 22:34:22,352 - root - INFO - distance: 0.007525252178311348\n",
            "2024-12-22 22:34:22,352 - root - INFO - Computed distance between ./data/.out/synthetized/Author1/Author1_0040_09.png, ./data/.out/synthetized/Author3/Author3_0008_03.png: 0.007525252178311348\n",
            "2024-12-22 22:34:24,825 - root - INFO - fuzzy parameters: 3.7083072662353516 shared centroids, 3.774752616882324 total centroids\n",
            "2024-12-22 22:34:24,826 - root - INFO - mean integral: 0.01466429140418768\n",
            "2024-12-22 22:34:24,826 - root - INFO - distance: 0.007397250737994909\n",
            "2024-12-22 22:34:24,827 - root - INFO - Computed distance between ./data/.out/synthetized/Author1/Author1_0040_09.png, ./data/.out/synthetized/Author3/Author3_0008_03.png: 0.007397250737994909\n",
            "2024-12-22 22:34:27,817 - root - INFO - fuzzy parameters: 3.6557400226593018 shared centroids, 3.725907325744629 total centroids\n",
            "2024-12-22 22:34:27,817 - root - INFO - mean integral: 0.014863581396639347\n",
            "2024-12-22 22:34:27,818 - root - INFO - distance: 0.007502434775233269\n",
            "2024-12-22 22:34:27,818 - root - INFO - Computed distance between ./data/.out/synthetized/Author1/Author1_0040_09.png, ./data/.out/synthetized/Author3/Author3_0008_03.png: 0.007502434775233269\n",
            "2024-12-22 22:34:32,421 - root - INFO - fuzzy parameters: 4.488320350646973 shared centroids, 4.6534881591796875 total centroids\n",
            "2024-12-22 22:34:32,422 - root - INFO - mean integral: 0.014770755544304848\n",
            "2024-12-22 22:34:32,422 - root - INFO - distance: 0.007518812082707882\n",
            "2024-12-22 22:34:32,423 - root - INFO - Computed distance between ./data/.out/synthetized/Author1/Author1_0040_09.png, ./data/.out/synthetized/Author3/Author3_0008_03.png: 0.007518812082707882\n",
            "2024-12-22 22:34:37,203 - root - INFO - fuzzy parameters: 3.6462607383728027 shared centroids, 3.7120399475097656 total centroids\n",
            "2024-12-22 22:34:37,204 - root - INFO - mean integral: 0.014869778417050838\n",
            "2024-12-22 22:34:37,205 - root - INFO - distance: 0.0075013525784015656\n",
            "2024-12-22 22:34:37,206 - root - INFO - Computed distance between ./data/.out/synthetized/Author1/Author1_0040_09.png, ./data/.out/synthetized/Author3/Author3_0008_03.png: 0.0075013525784015656\n",
            "2024-12-22 22:34:41,864 - root - INFO - fuzzy parameters: 3.640681743621826 shared centroids, 3.7201485633850098 total centroids\n",
            "2024-12-22 22:34:41,865 - root - INFO - mean integral: 0.01486176997423172\n",
            "2024-12-22 22:34:41,865 - root - INFO - distance: 0.007511107716709375\n",
            "2024-12-22 22:34:41,866 - root - INFO - Computed distance between ./data/.out/synthetized/Author1/Author1_0040_09.png, ./data/.out/synthetized/Author3/Author3_0008_03.png: 0.007511107716709375\n",
            "2024-12-22 22:34:45,338 - root - INFO - fuzzy parameters: 3.682922840118408 shared centroids, 3.7563769817352295 total centroids\n",
            "2024-12-22 22:34:45,339 - root - INFO - mean integral: 0.014742315746843815\n",
            "2024-12-22 22:34:45,339 - root - INFO - distance: 0.007443938869982958\n",
            "2024-12-22 22:34:45,340 - root - INFO - Computed distance between ./data/.out/synthetized/Author1/Author1_0040_09.png, ./data/.out/synthetized/Author3/Author3_0008_03.png: 0.007443938869982958\n",
            "2024-12-22 22:34:49,455 - root - INFO - fuzzy parameters: 3.6857995986938477 shared centroids, 3.753669023513794 total centroids\n",
            "2024-12-22 22:34:49,456 - root - INFO - mean integral: 0.01478634588420391\n",
            "2024-12-22 22:34:49,456 - root - INFO - distance: 0.007460619788616896\n",
            "2024-12-22 22:34:49,457 - root - INFO - Computed distance between ./data/.out/synthetized/Author1/Author1_0040_09.png, ./data/.out/synthetized/Author3/Author3_0008_03.png: 0.007460619788616896\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from source.packages import distance, clustering\n",
        "import logging\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Remove all handlers associated with the logger object\n",
        "for handler in logger.handlers[:]:\n",
        "    logger.removeHandler(handler)\n",
        "\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "console_handler.setFormatter(formatter)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "n_tiles = 6\n",
        "fcm_tollerance = 10**-8\n",
        "final_dim = 10\n",
        "n_clusters = 128\n",
        "\n",
        "# elenco tutte le opere\n",
        "my_list = []\n",
        "for root, dirs, files in os.walk(r\"./data/.out/synthetized\"):\n",
        "    for file in files:\n",
        "        if file.endswith(\".png\"):\n",
        "            my_list.append(os.path.join(root, file))\n",
        "\n",
        "# calcolo per 10 volte la distanza tra le due sintesi\n",
        "dist_list = []\n",
        "work_1 = np.random.choice(my_list)\n",
        "work_2 = np.random.choice(my_list)\n",
        "for i in range(10):\n",
        "\n",
        "    # open synthesis\n",
        "    with open(work_1, \"br\") as f:\n",
        "        values = f.read()\n",
        "    synth_1 = (\n",
        "        np.frombuffer(values, dtype=np.uint8)\n",
        "        .reshape(-1, n_tiles * n_tiles)\n",
        "        .astype(np.float32)\n",
        "        / 255.0\n",
        "    )\n",
        "    # open synthesis\n",
        "    with open(work_2, \"br\") as f:\n",
        "        values = f.read()\n",
        "    synth_2 = (\n",
        "        np.frombuffer(values, dtype=np.uint8)\n",
        "        .reshape(-1, n_tiles * n_tiles)\n",
        "        .astype(np.float32)\n",
        "        / 255.0\n",
        "    )\n",
        "    synth_merge = np.vstack((synth_1, synth_2))\n",
        "\n",
        "    # eseguo una pca\n",
        "    pca = PCA(n_components=final_dim)\n",
        "    synth_merge = pca.fit_transform(synth_merge)\n",
        "\n",
        "    # salvo la sintesi merge in un file temporaneo come float32 binario\n",
        "    with open(r\"./temp/synth_merge\", \"bw\") as f:\n",
        "        f.write(synth_merge.tobytes())\n",
        "    # estraggo un campione di n_clusters righe da synth_merge\n",
        "    synth_sample = np.random.choice(\n",
        "        synth_merge.shape[0], n_clusters, replace=False\n",
        "    )\n",
        "    synth_sample = synth_merge[synth_sample]\n",
        "    # aggiungo del noise\n",
        "    synth_sample += np.random.normal(0, 0.01, synth_sample.shape)\n",
        "    # salvo il campione in un file temporaneo come float32 binario (i centroidi iniziali)\n",
        "    with open(r\"./temp/synth_sample\", \"bw\") as f:\n",
        "        f.write(synth_sample.tobytes())\n",
        "\n",
        "    # costruisco il campione dei pesi, tutti uguali per i rispettivi synth\n",
        "    synth_weights = np.ones((synth_merge.shape[0]), dtype=np.float32)\n",
        "    # mi chiedo quale sia la sintesi con pi\u00f9 dati\n",
        "    synth_weights[: synth_1.shape[0]] = (\n",
        "        np.ones((synth_1.shape[0]), dtype=np.float32)\n",
        "        / synth_1.shape[0]\n",
        "    )\n",
        "    synth_weights[synth_1.shape[0] :] = (\n",
        "        np.ones((synth_2.shape[0]), dtype=np.float32)\n",
        "        / synth_2.shape[0]\n",
        "    )\n",
        "    # salvo i pesi in un file temporaneo come float32 binario\n",
        "    with open(r\"./temp/synth_weights\", \"bw\") as f:\n",
        "        f.write(synth_weights.tobytes())\n",
        "\n",
        "    # libero la ram (fondamentale per evitare memory error)\n",
        "    del values\n",
        "    del synth_1\n",
        "    del synth_2\n",
        "    del synth_merge  # ora presente in ./temp/synth_merge\n",
        "    del synth_sample  # ora presente in ./temp/synth_sample\n",
        "    del synth_weights  # ora presente in ./temp/synth_weights\n",
        "\n",
        "    # eseguo il clustering fcm\n",
        "    try:\n",
        "        clustering.fcm(\n",
        "            logger,\n",
        "            r\"./temp/synth_merge\",\n",
        "            r\"./temp/synth_weights\",\n",
        "            r\"./temp/synth_sample\",\n",
        "            r\"./temp/centroids\",\n",
        "            final_dim,\n",
        "            fcm_tollerance,\n",
        "            r\"./logs/fcm.log\",\n",
        "        )\n",
        "    except SyntaxError:\n",
        "        logger.critical(\"Implementation error!\")\n",
        "        exit()\n",
        "    except ValueError:\n",
        "        logger.error(\"Unvalid inputs\")\n",
        "        exit()\n",
        "    except Exception:\n",
        "        logger.error(\"Unexpected error\")\n",
        "        exit()\n",
        "\n",
        "    # uso i centroidi in \"./temp/centroids\" per calcolare la distanza tra i due synth\n",
        "    # load data\n",
        "    with open(work_1, \"br\") as f:\n",
        "        values = f.read()\n",
        "    synth_1 = (\n",
        "        np.frombuffer(values, dtype=np.uint8)\n",
        "        .reshape(-1, n_tiles * n_tiles)\n",
        "        .astype(np.float32)\n",
        "        / 255.0\n",
        "    )\n",
        "    with open(work_2, \"br\") as f:\n",
        "        values = f.read()\n",
        "    synth_2 = (\n",
        "        np.frombuffer(values, dtype=np.uint8)\n",
        "        .reshape(-1, n_tiles * n_tiles)\n",
        "        .astype(np.float32)\n",
        "        / 255.0\n",
        "    )\n",
        "    # riduco la dimensionalit\u00e0 usando la stessa pca precedente\n",
        "    synth_1 = pca.transform(synth_1)\n",
        "    synth_2 = pca.transform(synth_2)\n",
        "\n",
        "    weights_1 = (\n",
        "        np.ones((synth_1.shape[0]), dtype=np.float32)\n",
        "        / synth_1.shape[0]\n",
        "    )\n",
        "    weights_2 = (\n",
        "        np.ones((synth_2.shape[0]), dtype=np.float32)\n",
        "        / synth_2.shape[0]\n",
        "    )\n",
        "    with open(r\"./temp/centroids\", \"br\") as f:\n",
        "        values = f.read()\n",
        "    centroids = np.frombuffer(values, dtype=np.float32).reshape(\n",
        "        n_clusters, final_dim\n",
        "    )\n",
        "    # compute distance\n",
        "    try:\n",
        "        dist = distance.compute_distance(\n",
        "            logger,\n",
        "            synth_1,\n",
        "            synth_2,\n",
        "            weights_1,\n",
        "            weights_2,\n",
        "            centroids,\n",
        "        )\n",
        "    except SyntaxError:\n",
        "        logger.critical(\"Implementation error!\")\n",
        "        exit()\n",
        "    except ValueError:\n",
        "        logger.error(\"Unvalid inputs\")\n",
        "        exit()\n",
        "    except Exception:\n",
        "        logger.error(\"Unexpected error\")\n",
        "        exit()\n",
        "    else:\n",
        "        logger.info(f\"Computed distance between {work_1}, {work_2}: {dist}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.std(dist_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Genero i dati\n",
        "import torch\n",
        "import numpy as np\n",
        "from source.packages import clustering\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "n_dimensions = 2\n",
        "\n",
        "# genero il set di dati di A in R2\n",
        "data_A1 = torch.randn(5, n_dimensions) / 2 + (torch.rand(n_dimensions) - 0.5)*20\n",
        "weights_A1 = torch.rand((data_A1.shape[0],))*0.8+0.6\n",
        "data_A2 = torch.randn(5, n_dimensions) + (torch.rand(n_dimensions) - 0.5)*10\n",
        "weights_A2 = torch.rand((data_A2.shape[0],))*0.8+0.6\n",
        "data_A3 = torch.randn(5, n_dimensions) * 2 + (torch.rand(n_dimensions) - 0.5)*20\n",
        "weights_A3 = torch.rand((data_A3.shape[0],))*0.8+0.6\n",
        "data_A4 = torch.randn(15, n_dimensions) * 2 + (torch.rand(n_dimensions) - 0.5)*20\n",
        "weights_A4 = torch.rand((data_A4.shape[0],))*0.8+0.6\n",
        "data_A5 = torch.randn(15, n_dimensions) / 2 + (torch.rand(n_dimensions) - 0.5)*20\n",
        "weights_A5 = torch.rand((data_A5.shape[0],))*0.8+0.6\n",
        "data_A6 = torch.randn(25, n_dimensions) + (torch.rand(n_dimensions) - 0.5)*20\n",
        "weights_A6 = torch.rand((data_A6.shape[0],))*0.8+0.6\n",
        "\n",
        "# genero il set di dati di B in R3\n",
        "data_B1 = torch.randn(5, n_dimensions) / 2 + (torch.rand(n_dimensions) - 0.5)*20 \n",
        "weights_B1 = torch.rand((data_B1.shape[0],))*0.8+0.6\n",
        "data_B2 = torch.randn(5, n_dimensions) + (torch.rand(n_dimensions) - 0.5)*20\n",
        "weights_B2 = torch.rand((data_B2.shape[0],))*0.8+0.6\n",
        "data_B3 = torch.randn(5, n_dimensions) * 2 + (torch.rand(n_dimensions) - 0.5)*20\n",
        "weights_B3 = torch.rand((data_B3.shape[0],))*0.8+0.6\n",
        "data_B4 = torch.randn(15, n_dimensions) / 2 + (torch.rand(n_dimensions) - 0.5)*20\n",
        "weights_B4 = torch.rand((data_B4.shape[0],))*0.8+0.6\n",
        "data_B5 = torch.randn(15, n_dimensions) * 2 + (torch.rand(n_dimensions) - 0.5)*20\n",
        "weights_B5 = torch.rand((data_B5.shape[0],))*0.8+0.6\n",
        "data_B6 = torch.randn(25, n_dimensions) + (torch.rand(n_dimensions) - 0.5)*20\n",
        "weights_B6 = torch.rand((data_B6.shape[0],))*0.8+0.6\n",
        "\n",
        "# aggiungo noise\n",
        "noise_A = (torch.rand((5, n_dimensions)) - 0.5)*20\n",
        "nweights_A = torch.ones((noise_A.shape[0],))\n",
        "noise_B = (torch.rand((5, n_dimensions)) - 0.5)*20\n",
        "nweights_B = torch.ones((noise_B.shape[0],))\n",
        "\n",
        "# unisco i dati\n",
        "data_A = torch.vstack([data_A1, data_A2, data_A3, data_A4, data_A5, data_A6, noise_A]).to(torch.float32)\n",
        "weights_A = torch.concatenate([weights_A1, weights_A2, weights_A3, weights_A4, weights_A5, weights_A6, nweights_A]).to(torch.float32)\n",
        "data_B = torch.vstack([data_B1, data_B2, data_B3, data_B4, data_B5, data_B6, noise_B]).to(torch.float32)\n",
        "weights_B = torch.concatenate([weights_B1, weights_B2, weights_B3, weights_B4, weights_B5, weights_B6, nweights_B]).to(torch.float32)\n",
        "\n",
        "# ottengo i dati uniti\n",
        "data = torch.vstack([data_A, data_B])\n",
        "weights = torch.concatenate([weights_A, weights_B])\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(weights.shape, weights.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot data\n",
        "from plotly import graph_objects as go\n",
        "\n",
        "# scatter di data_A1, uso weights_A1 per la dimensione dei punti\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=data_A[:, 0].cpu().numpy(),\n",
        "    y=data_A[:, 1].cpu().numpy(),\n",
        "    mode='markers',\n",
        "    marker=dict(size=weights_A.cpu().numpy()*10, color='red'),\n",
        "    name='Data A',\n",
        "    opacity=1.0\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=data_B[:, 0].cpu().numpy(),\n",
        "    y=data_B[:, 1].cpu().numpy(),\n",
        "    mode='markers',\n",
        "    marker=dict(size=weights_B.cpu().numpy()*10, color='blue'),\n",
        "    name='Data B',\n",
        "    opacity=1.0\n",
        "))\n",
        "\n",
        "# le proporzioni della griglia devono essere 1:1\n",
        "fig.update_layout(\n",
        "    width=800,\n",
        "    height=800,\n",
        "    xaxis=dict(scaleanchor=\"y\", scaleratio=1),\n",
        "    yaxis=dict(scaleanchor=\"x\", scaleratio=1)\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calcolo il clustering fcm\n",
        "n_centroids = 15\n",
        "\n",
        "with open(r\"./temp/data\", \"bw\") as f:\n",
        "    f.write(data.cpu().numpy().tobytes())\n",
        "\n",
        "with open(r\"./temp/weights\", \"bw\") as f:\n",
        "    f.write(weights.cpu().numpy().tobytes())\n",
        "\n",
        "init_centroids = torch.rand(n_centroids,n_dimensions).to(torch.float32)\n",
        "with open(r\"./temp/centroids\", \"bw\") as f:\n",
        "    f.write(init_centroids.cpu().numpy().tobytes())\n",
        "\n",
        "with open(r\"./temp/output\", \"bw\") as f:\n",
        "    pass\n",
        "\n",
        "clustering.fcm(\n",
        "    logger,\n",
        "    r\"./temp/data\",\n",
        "    r\"./temp/weights\",\n",
        "    r\"./temp/centroids\",\n",
        "    r\"./temp/output\",\n",
        "    n_dimensions,\n",
        "    10**-9,\n",
        "    r\"./logs/fcm.log\",\n",
        ")\n",
        "\n",
        "with open(r\"./temp/output\", \"br\") as f:\n",
        "    buf = f.read()\n",
        "    centroids = torch.from_numpy(np.frombuffer(buf, dtype=np.float32)).reshape(-1,data.shape[1]).to(torch.float32)\n",
        "\n",
        "# compute distances\n",
        "d_distances = torch.cdist(data, centroids, p=2)**2\n",
        "# compute mu\n",
        "mu = 1 / (d_distances * torch.sum(1 / d_distances, dim=1, keepdim=True))\n",
        "mu[torch.isnan(mu)] = 1\n",
        "mu = mu / torch.sum(mu, dim=1, keepdim=True)\n",
        "\n",
        "# compute weights\n",
        "p = torch.einsum('ij, i -> j', mu, weights) / torch.sum(weights, dim=0)\n",
        "# compute sigma\n",
        "s = torch.einsum('ij, i -> j', mu**2, weights) / torch.einsum('ij, i -> j', mu, weights)\n",
        "# compute E\n",
        "E = torch.einsum('ij, i -> j', mu**2 * d_distances, weights) / torch.einsum('ij, i -> j', mu**2, weights)\n",
        "\n",
        "# compute mu of cluster\n",
        "measure = s * E ** (n_dimensions/2)\n",
        "\n",
        "# show data with centroids\n",
        "fig = go.Figure()\n",
        "# make a dict with f\"centroid {j}\":mu[i,j]\n",
        "myDict = {f\"centroid {j}\":mu[:,j].cpu().numpy() for j in range(n_centroids)}\n",
        "fig.add_trace(go.Scatter(\n",
        "\n",
        "    x=data_A[:, 0].cpu().numpy(),\n",
        "    y=data_A[:, 1].cpu().numpy(),\n",
        "    mode='markers',\n",
        "    marker=dict(size=weights_A.cpu().numpy()*10, color='red'),\n",
        "    name='Data A',\n",
        "    opacity=1.0,\n",
        "))\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=data_B[:, 0].cpu().numpy(),\n",
        "    y=data_B[:, 1].cpu().numpy(),\n",
        "    mode='markers',\n",
        "    marker=dict(size=weights_B.cpu().numpy()*10, color='blue'),\n",
        "    name='Data B',\n",
        "    opacity=1.0\n",
        "))\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=centroids[:, 0].cpu().numpy(),\n",
        "    y=centroids[:, 1].cpu().numpy(),\n",
        "    mode='markers',\n",
        "    marker=dict(size=10, color='black', symbol='x'),\n",
        "    name='Centroids',\n",
        "    opacity=1.0\n",
        "))\n",
        "# aggiungo delle circonferenze di raggio measure**(1/n_dimensions)\n",
        "for i in range(n_centroids):\n",
        "    fig.add_shape(\n",
        "        type='circle',\n",
        "        xref='x',\n",
        "        yref='y',\n",
        "        x0=centroids[i, 0].item() - measure[i].item()**(1/n_dimensions),\n",
        "        y0=centroids[i, 1].item() - measure[i].item()**(1/n_dimensions),\n",
        "        x1=centroids[i, 0].item() + measure[i].item()**(1/n_dimensions),\n",
        "        y1=centroids[i, 1].item() + measure[i].item()**(1/n_dimensions),\n",
        "        line=dict(color='black', width=1)\n",
        "    )\n",
        "\n",
        "# voglio l'immagine grande\n",
        "fig.update_layout(\n",
        "    width=800,\n",
        "    height=800,\n",
        "    xaxis=dict(scaleanchor=\"y\", scaleratio=1),\n",
        "    yaxis=dict(scaleanchor=\"x\", scaleratio=1)\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "# calcolo la distanza tra A e B\n",
        "p_A = torch.einsum('ij, i -> j', mu[:data_A.shape[0], :], weights[:data_A.shape[0]]) / torch.sum(weights[:data_A.shape[0]])\n",
        "p_B = torch.einsum('ij, i -> j', mu[data_A.shape[0]:, :], weights[data_A.shape[0]:]) / torch.sum(weights[data_A.shape[0]:])\n",
        "value = (p_A - p_B) / (p_A + p_B)\n",
        "\n",
        "# calcolo la media integrale\n",
        "integral = torch.sum(measure * value**2) / torch.sum(measure)\n",
        "\n",
        "# calcolo la cardinalit\u00e0 di |D_A n D_B| e |D_A u D_B|\n",
        "c_A = torch.max(mu[:data_A.shape[0], :], dim=0)[0]  # esistenza del cluster nel dizionario di A\n",
        "c_B = torch.max(mu[data_A.shape[0]:, :], dim=0)[0]  # esistenza del cluster nel dizionario di B\n",
        "card_inter = torch.sum(torch.minimum(c_A, c_B))\n",
        "card_union = torch.sum(torch.maximum(c_A, c_B))\n",
        "Jaccard_index = card_inter / card_union\n",
        "\n",
        "distance = (1 + Jaccard_index)**(-1) * integral\n",
        "print(f\"{distance=}\")\n",
        "print(f\"\\t{Jaccard_index=}\")\n",
        "print(f\"\\t\\t{card_inter=}\", f\"{card_union=}\")\n",
        "print(f\"\\t\\t\\t{c_A=}\")\n",
        "print(f\"\\t\\t\\t{c_B=}\")\n",
        "print(f\"\\t{integral=}\")\n",
        "print(f\"\\t\\t{value=}\")\n",
        "print(f\"\\t\\t\\t{p_A=}\")\n",
        "print(f\"\\t\\t\\t{p_B=}\")\n",
        "\n",
        "\n",
        "# Plot dei cluster:\n",
        "colors = [(0, 0, 1), (1, 0, 0)]\n",
        "n_bins = 256  # numero di intervalli nella colormap\n",
        "cmap_name = 'membership_colormap'\n",
        "my_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "# add points\n",
        "plt.scatter(data_A.cpu().numpy()[:,0], data_A.cpu().numpy()[:,1], weights_A.cpu().numpy(), c='red')\n",
        "plt.scatter(data_B.cpu().numpy()[:,0], data_B.cpu().numpy()[:,1], weights_B.cpu().numpy(), c='blue')\n",
        "for i in range(n_centroids):\n",
        "    s = (value[i] + 1) / 2\n",
        "    circle = plt.Circle(\n",
        "        centroids[i].cpu(),\n",
        "        measure[i].item()**(1/n_dimensions),\n",
        "        color=my_cmap(s.item()),\n",
        "        alpha=0.8\n",
        "    )\n",
        "    ax.add_artist(circle)\n",
        "norm = plt.Normalize(vmin=-1, vmax=1)\n",
        "sm = plt.cm.ScalarMappable(cmap=my_cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "cbar = fig.colorbar(sm, ax=ax)  # Associa la colorbar all'asse 'ax'\n",
        "cbar.set_label('integrand')\n",
        "cbar.set_ticks([-1, 0, 1])  # Imposta i tick manualmente\n",
        "cbar.set_ticklabels(['B', '0', 'A'])  # Imposta le etichette dei tick\n",
        "plt.xlim(-20, 20)\n",
        "plt.ylim(-20, 20)\n",
        "plt.savefig('./data/fcm_comparison.png', dpi=600)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# using kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=n_centroids)\n",
        "kmeans.fit(data.cpu().numpy(), sample_weight=weights.cpu().numpy())\n",
        "centroids = torch.from_numpy(kmeans.cluster_centers_).to(torch.float32)\n",
        "\n",
        "# compute distances\n",
        "d_distances = torch.cdist(data, centroids, p=2)**2\n",
        "# compute mu: m[i,j] = 1 if label is 1 0 otherwise\n",
        "mu = torch.zeros(data.shape[0], n_centroids)\n",
        "for i in range(data.shape[0]):\n",
        "    mu[i, kmeans.labels_[i]] = 1\n",
        "\n",
        "# compute weights\n",
        "p = torch.einsum('ij, i -> j', mu, weights) / torch.sum(weights, dim=0)\n",
        "# compute sigma\n",
        "s = torch.einsum('ij, i -> j', mu**2, weights) / torch.einsum('ij, i -> j', mu, weights)\n",
        "# compute E\n",
        "E = torch.einsum('ij, i -> j', mu**2 * d_distances, weights) / torch.einsum('ij, i -> j', mu**2, weights)\n",
        "\n",
        "# compute mu of cluster\n",
        "measure = s * E ** (n_dimensions/2)\n",
        "\n",
        "fig = go.Figure()\n",
        "# make a dict with f\"centroid {j}\":mu[i,j]\n",
        "myDict = {f\"centroid {j}\":mu[:,j].cpu().numpy() for j in range(n_centroids)}\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=data_A[:, 0].cpu().numpy(),\n",
        "    y=data_A[:, 1].cpu().numpy(),\n",
        "    mode='markers',\n",
        "    marker=dict(size=weights_A.cpu().numpy()*10, color='red'),\n",
        "    name='Data A',\n",
        "    opacity=1.0,\n",
        "))\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=data_B[:, 0].cpu().numpy(),\n",
        "    y=data_B[:, 1].cpu().numpy(),\n",
        "    mode='markers',\n",
        "    marker=dict(size=weights_B.cpu().numpy()*10, color='blue'),\n",
        "    name='Data B',\n",
        "    opacity=1.0\n",
        "))\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=centroids[:, 0].cpu().numpy(),\n",
        "    y=centroids[:, 1].cpu().numpy(),\n",
        "    mode='markers',\n",
        "    marker=dict(size=10, color='black', symbol='x'),\n",
        "    name='Centroids',\n",
        "    opacity=1.0\n",
        "))\n",
        "# aggiungo delle circonferenze di raggio measure**(1/n_dimensions)\n",
        "for i in range(n_centroids):\n",
        "    fig.add_shape(\n",
        "        type='circle',\n",
        "        xref='x',\n",
        "        yref='y',\n",
        "        x0=centroids[i, 0].item() - measure[i].item()**(1/n_dimensions),\n",
        "        y0=centroids[i, 1].item() - measure[i].item()**(1/n_dimensions),\n",
        "        x1=centroids[i, 0].item() + measure[i].item()**(1/n_dimensions),\n",
        "        y1=centroids[i, 1].item() + measure[i].item()**(1/n_dimensions),\n",
        "        line=dict(color='black', width=1)\n",
        "    )\n",
        "\n",
        "# voglio l'immagine grande\n",
        "fig.update_layout(\n",
        "    width=800,\n",
        "    height=800,\n",
        "    xaxis=dict(scaleanchor=\"y\", scaleratio=1),\n",
        "    yaxis=dict(scaleanchor=\"x\", scaleratio=1)\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# calcolo la distanza tra A e B\n",
        "p_A = torch.einsum('ij, i -> j', mu[:data_A.shape[0], :], weights[:data_A.shape[0]]) / torch.sum(weights[:data_A.shape[0]])\n",
        "p_B = torch.einsum('ij, i -> j', mu[data_A.shape[0]:, :], weights[data_A.shape[0]:]) / torch.sum(weights[data_A.shape[0]:])\n",
        "value = (p_A - p_B) / (p_A + p_B)\n",
        "\n",
        "# calcolo la media integrale\n",
        "integral = torch.sum(measure * value**2) / torch.sum(measure)\n",
        "\n",
        "# calcolo la cardinalit\u00e0 di |D_A n D_B| e |D_A u D_B|\n",
        "c_A = torch.max(mu[:data_A.shape[0], :], dim=0)[0]  # esistenza del cluster nel dizionario di A\n",
        "c_B = torch.max(mu[data_A.shape[0]:, :], dim=0)[0]  # esistenza del cluster nel dizionario di B\n",
        "card_inter = torch.sum(torch.minimum(c_A, c_B))\n",
        "card_union = torch.sum(torch.maximum(c_A, c_B))\n",
        "Jaccard_index = card_inter / card_union\n",
        "\n",
        "distance = (1 + Jaccard_index)**(-1) * integral\n",
        "print(f\"{distance=}\")\n",
        "print(f\"\\t{Jaccard_index=}\")\n",
        "print(f\"\\t\\t{card_inter=}\", f\"{card_union=}\")\n",
        "print(f\"\\t\\t\\t{c_A=}\")\n",
        "print(f\"\\t\\t\\t{c_B=}\")\n",
        "print(f\"\\t{integral=}\")\n",
        "print(f\"\\t\\t{value=}\")\n",
        "print(f\"\\t\\t\\t{p_A=}\")\n",
        "print(f\"\\t\\t\\t{p_B=}\")\n",
        "\n",
        "# Plot dei cluster:\n",
        "colors = [(0, 0, 1), (1, 0, 0)]\n",
        "n_bins = 256  # numero di intervalli nella colormap\n",
        "cmap_name = 'membership_colormap'\n",
        "my_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "# add points\n",
        "plt.scatter(data_A.cpu().numpy()[:,0], data_A.cpu().numpy()[:,1], weights_A.cpu().numpy(), c='red')\n",
        "plt.scatter(data_B.cpu().numpy()[:,0], data_B.cpu().numpy()[:,1], weights_B.cpu().numpy(), c='blue')\n",
        "for i in range(n_centroids):\n",
        "    s = (value[i] + 1) / 2\n",
        "    circle = plt.Circle(\n",
        "        centroids[i].cpu(),\n",
        "        measure[i].item()**(1/n_dimensions),\n",
        "        color=my_cmap(s.item()),\n",
        "        alpha=0.8\n",
        "    )\n",
        "    ax.add_artist(circle)\n",
        "norm = plt.Normalize(vmin=-1, vmax=1)\n",
        "sm = plt.cm.ScalarMappable(cmap=my_cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "cbar = fig.colorbar(sm, ax=ax)  # Associa la colorbar all'asse 'ax'\n",
        "cbar.set_label('integrand')\n",
        "cbar.set_ticks([-1, 0, 1])  # Imposta i tick manualmente\n",
        "cbar.set_ticklabels(['B', '0', 'A'])  # Imposta le etichette dei tick\n",
        "plt.xlim(-20, 20)\n",
        "plt.ylim(-20, 20)\n",
        "plt.savefig('./data/kmeans_comparison.png', dpi=600)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# uso lo box per clusterizzare i dati: uso i 4 quadranti\n",
        "# calcolo i centroidi\n",
        "centroids = torch.zeros(n_centroids, n_dimensions)\n",
        "for i in range(n_dimensions):\n",
        "    for j in range(n_dimensions):\n",
        "        centroids[i*n_dimensions+j] = torch.tensor([20*(i-0.5), 20*(j-0.5)])\n",
        "\n",
        "# i label sono stabiliti in base al segno dei dati\n",
        "mu = torch.zeros(data.shape[0], n_centroids)\n",
        "for i in range(data.shape[0]):\n",
        "    if data[i,0] < 0 and data[i,1] < 0:\n",
        "        mu[i,0] = 1\n",
        "    elif data[i, 0] < 0:\n",
        "        mu[i,1] = 1\n",
        "    elif data[i, 1] < 0:\n",
        "        mu[i,2] = 1\n",
        "    else:\n",
        "        mu[i,3] = 1\n",
        "\n",
        "measure = torch.ones(n_centroids)\n",
        "\n",
        "fig = go.Figure()\n",
        "# make a dict with f\"centroid {j}\":mu[i,j]\n",
        "myDict = {f\"centroid {j}\":mu[:,j].cpu().numpy() for j in range(n_centroids)}\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=data_A[:, 0].cpu().numpy(),\n",
        "    y=data_A[:, 1].cpu().numpy(),\n",
        "    mode='markers',\n",
        "    marker=dict(size=weights_A.cpu().numpy()*10, color='red'),\n",
        "    name='Data A',\n",
        "    opacity=1.0,\n",
        "))\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=data_B[:, 0].cpu().numpy(),\n",
        "    y=data_B[:, 1].cpu().numpy(),\n",
        "    mode='markers',\n",
        "    marker=dict(size=weights_B.cpu().numpy()*10, color='blue'),\n",
        "    name='Data B',\n",
        "    opacity=1.0\n",
        "))\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=centroids[:, 0].cpu().numpy(),\n",
        "    y=centroids[:, 1].cpu().numpy(),\n",
        "    mode='markers',\n",
        "    marker=dict(size=10, color='black', symbol='x'),\n",
        "    name='Centroids',\n",
        "    opacity=1.0\n",
        "))\n",
        "# aggiungo una linea nera orizzontale da -20 a 20 e verticale da -20 a 20\n",
        "fig.add_shape(\n",
        "    type='line',\n",
        "    x0=-20,\n",
        "    y0=0,\n",
        "    x1=20,\n",
        "    y1=0,\n",
        "    line=dict(color='black', width=2)\n",
        ")\n",
        "fig.add_shape(\n",
        "    type='line',\n",
        "    x0=0,\n",
        "    y0=-20,\n",
        "    x1=0,\n",
        "    y1=20,\n",
        "    line=dict(color='black', width=2)\n",
        ")\n",
        "\n",
        "# voglio l'immagine grande\n",
        "fig.update_layout(\n",
        "    width=800,\n",
        "    height=800,\n",
        "    xaxis=dict(scaleanchor=\"y\", scaleratio=1),\n",
        "    yaxis=dict(scaleanchor=\"x\", scaleratio=1)\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# calcolo la distanza tra A e B\n",
        "p_A = torch.einsum('ij, i -> j', mu[:data_A.shape[0], :], weights[:data_A.shape[0]]) / torch.sum(weights[:data_A.shape[0]])\n",
        "p_B = torch.einsum('ij, i -> j', mu[data_A.shape[0]:, :], weights[data_A.shape[0]:]) / torch.sum(weights[data_A.shape[0]:])\n",
        "value = (p_A - p_B) / (p_A + p_B)\n",
        "\n",
        "# calcolo la media integrale\n",
        "integral = torch.sum(measure * value**2) / torch.sum(measure)\n",
        "\n",
        "# calcolo la cardinalit\u00e0 di |D_A n D_B| e |D_A u D_B|\n",
        "c_A = torch.max(mu[:data_A.shape[0], :], dim=0)[0]  # esistenza del cluster nel dizionario di A\n",
        "c_B = torch.max(mu[data_A.shape[0]:, :], dim=0)[0]  # esistenza del cluster nel dizionario di B\n",
        "card_inter = torch.sum(torch.minimum(c_A, c_B))\n",
        "card_union = torch.sum(torch.maximum(c_A, c_B))\n",
        "Jaccard_index = card_inter / card_union\n",
        "\n",
        "distance = (1 + Jaccard_index)**(-1) * integral\n",
        "print(f\"{distance=}\")\n",
        "print(f\"\\t{Jaccard_index=}\")\n",
        "print(f\"\\t\\t{card_inter=}\", f\"{card_union=}\")\n",
        "print(f\"\\t\\t\\t{c_A=}\")\n",
        "print(f\"\\t\\t\\t{c_B=}\")\n",
        "print(f\"\\t{integral=}\")\n",
        "print(f\"\\t\\t{value=}\")\n",
        "print(f\"\\t\\t\\t{p_A=}\")\n",
        "print(f\"\\t\\t\\t{p_B=}\")\n",
        "\n",
        "# Plot dei cluster:\n",
        "colors = [(0, 0, 1), (1, 0, 0)]\n",
        "n_bins = 256  # numero di intervalli nella colormap\n",
        "cmap_name = 'membership_colormap'\n",
        "my_cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "# add points\n",
        "plt.scatter(data_A.cpu().numpy()[:,0], data_A.cpu().numpy()[:,1], weights_A.cpu().numpy(), c='red')\n",
        "plt.scatter(data_B.cpu().numpy()[:,0], data_B.cpu().numpy()[:,1], weights_B.cpu().numpy(), c='blue')\n",
        "for i in range(n_centroids):\n",
        "    s = (value[i] + 1) / 2\n",
        "    rectangle = plt.Rectangle(\n",
        "        (centroids[i, 0].cpu() - 10, centroids[i, 1].cpu() - 10),\n",
        "        20, 20,\n",
        "        color=my_cmap(s.item()),\n",
        "        alpha=0.8\n",
        "    )\n",
        "    ax.add_artist(rectangle)\n",
        "norm = plt.Normalize(vmin=-1, vmax=1)\n",
        "sm = plt.cm.ScalarMappable(cmap=my_cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "cbar = fig.colorbar(sm, ax=ax)  # Associa la colorbar all'asse 'ax'\n",
        "cbar.set_label('integrand')\n",
        "cbar.set_ticks([-1, 0, 1])  # Imposta i tick manualmente\n",
        "cbar.set_ticklabels(['B', '0', 'A'])  # Imposta le etichette dei tick\n",
        "plt.xlim(-20, 20)\n",
        "plt.ylim(-20, 20)\n",
        "plt.savefig('./data/box_comparison.png', dpi=600)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stability comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# genero i dati:\n",
        "import torch\n",
        "import numpy as np\n",
        "from source.packages import clustering\n",
        "from sklearn.cluster import KMeans\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "n_dimensions = 16\n",
        "n_data = 500\n",
        "n_centroids = 8\n",
        "noise_intensity = 0.2\n",
        "\n",
        "data_A = torch.randn(int(n_data * (1 - noise_intensity)), n_dimensions) + torch.ones(n_dimensions)\n",
        "data_B = torch.randn(int(n_data * (1 - noise_intensity)), n_dimensions) - torch.ones(n_dimensions)\n",
        "weights_A = torch.rand((data_A.shape[0],))*0.8+0.6\n",
        "weights_B = torch.rand((data_B.shape[0],))*0.8+0.6\n",
        "noise_A = (torch.rand(int(n_data * noise_intensity), n_dimensions) - 0.5)*10\n",
        "noise_B = (torch.rand(int(n_data * noise_intensity), n_dimensions) - 0.5)*10\n",
        "weights_noise_A = torch.ones((noise_A.shape[0],))\n",
        "weights_noise_B = torch.ones((noise_B.shape[0],))\n",
        "\n",
        "data_A = torch.vstack([data_A, noise_A]).to(torch.float32)\n",
        "data_B = torch.vstack([data_B, noise_B]).to(torch.float32)\n",
        "weights_A = torch.cat([weights_A, weights_noise_A])\n",
        "weights_B = torch.cat([weights_B, weights_noise_B])\n",
        "data = torch.vstack([data_A, data_B])\n",
        "weights = torch.cat([weights_A, weights_B])\n",
        "\n",
        "# normalizzo data in -1,1\n",
        "data -= data.min(dim=0, keepdim=True)[0]\n",
        "data /= data.norm(dim=1).max()\n",
        "\n",
        "# clustering con fcm\n",
        "with open(r\"./temp/data\", \"bw\") as f:\n",
        "    f.write(data.cpu().numpy().tobytes())\n",
        "with open(r\"./temp/weights\", \"bw\") as f:\n",
        "    f.write(weights.cpu().numpy().tobytes())\n",
        "init_centroids = torch.rand(n_centroids,n_dimensions).to(torch.float32)\n",
        "with open(r\"./temp/centroids\", \"bw\") as f:\n",
        "    f.write(init_centroids.cpu().numpy().tobytes())\n",
        "with open(r\"./temp/output\", \"bw\") as f:\n",
        "    pass\n",
        "clustering.fcm(\n",
        "    logger,\n",
        "    r\"./temp/data\",\n",
        "    r\"./temp/weights\",\n",
        "    r\"./temp/centroids\",\n",
        "    r\"./temp/output\",\n",
        "    n_dimensions,\n",
        "    10**-9,\n",
        "    r\"./logs/fcm.log\",\n",
        ")\n",
        "with open(r\"./temp/output\", \"br\") as f:\n",
        "    buf = f.read()\n",
        "    centroids = torch.from_numpy(np.frombuffer(buf, dtype=np.float32)).reshape(-1,data.shape[1]).to(torch.float32)\n",
        "d_distances = torch.cdist(data, centroids, p=2)**2\n",
        "mu = 1 / (d_distances * torch.sum(1 / d_distances, dim=1, keepdim=True))\n",
        "mu[torch.isnan(mu)] = 1\n",
        "mu = mu / torch.sum(mu, dim=1, keepdim=True)\n",
        "p = torch.einsum('ij, i -> j', mu, weights) / torch.sum(weights, dim=0)\n",
        "s = torch.einsum('ij, i -> j', mu**2, weights) / torch.einsum('ij, i -> j', mu, weights)\n",
        "E = torch.einsum('ij, i -> j', mu**2 * d_distances, weights) / torch.einsum('ij, i -> j', mu**2, weights)\n",
        "measure = s * E ** (n_dimensions/2)\n",
        "p_A = torch.einsum('ij, i -> j', mu[:data_A.shape[0], :], weights[:data_A.shape[0]]) / torch.sum(weights[:data_A.shape[0]])\n",
        "p_B = torch.einsum('ij, i -> j', mu[data_A.shape[0]:, :], weights[data_A.shape[0]:]) / torch.sum(weights[data_A.shape[0]:])\n",
        "value = (p_A - p_B) / (p_A + p_B)\n",
        "integral = torch.sum(measure * value**2) / torch.sum(measure)\n",
        "c_A = torch.max(mu[:data_A.shape[0], :], dim=0)[0]\n",
        "c_B = torch.max(mu[data_A.shape[0]:, :], dim=0)[0]\n",
        "card_inter = torch.sum(torch.minimum(c_A, c_B))\n",
        "card_union = torch.sum(torch.maximum(c_A, c_B))\n",
        "Jaccard_index = card_inter / card_union\n",
        "distance = (1 + Jaccard_index)**(-1) * integral\n",
        "print(f\"{distance=}\")\n",
        "print(f\"\\t{Jaccard_index=}\")\n",
        "print(f\"\\t\\t{card_inter=}\", f\"{card_union=}\")\n",
        "print(f\"\\t\\t\\t{c_A=}\")\n",
        "print(f\"\\t\\t\\t{c_B=}\")\n",
        "print(f\"\\t{integral=}\")\n",
        "print(f\"\\t\\t{value=}\")\n",
        "print(f\"\\t\\t\\t{p_A=}\")\n",
        "print(f\"\\t\\t\\t{p_B=}\")\n",
        "print()\n",
        "print()\n",
        "\n",
        "# kmeans\n",
        "kmeans = KMeans(n_clusters=n_centroids)\n",
        "kmeans.fit(data.cpu().numpy(), sample_weight=weights.cpu().numpy())\n",
        "centroids = torch.from_numpy(kmeans.cluster_centers_).to(torch.float32)\n",
        "d_distances = torch.cdist(data, centroids, p=2)**2\n",
        "mu = torch.zeros(data.shape[0], n_centroids)\n",
        "for i in range(data.shape[0]):\n",
        "    mu[i, kmeans.labels_[i]] = 1\n",
        "p = torch.einsum('ij, i -> j', mu, weights) / torch.sum(weights, dim=0)\n",
        "s = torch.einsum('ij, i -> j', mu**2, weights) / torch.einsum('ij, i -> j', mu, weights)\n",
        "E = torch.einsum('ij, i -> j', mu**2 * d_distances, weights) / torch.einsum('ij, i -> j', mu**2, weights)\n",
        "measure = s * E ** (n_dimensions/2)\n",
        "p_A = torch.einsum('ij, i -> j', mu[:data_A.shape[0], :], weights[:data_A.shape[0]]) / torch.sum(weights[:data_A.shape[0]])\n",
        "p_B = torch.einsum('ij, i -> j', mu[data_A.shape[0]:, :], weights[data_A.shape[0]:]) / torch.sum(weights[data_A.shape[0]:])\n",
        "value = (p_A - p_B) / (p_A + p_B)\n",
        "integral = torch.sum(measure * value**2) / torch.sum(measure)\n",
        "c_A = torch.max(mu[:data_A.shape[0], :], dim=0)[0]\n",
        "c_B = torch.max(mu[data_A.shape[0]:, :], dim=0)[0]\n",
        "card_inter = torch.sum(torch.minimum(c_A, c_B))\n",
        "card_union = torch.sum(torch.maximum(c_A, c_B))\n",
        "Jaccard_index = card_inter / card_union\n",
        "distance = (1 + Jaccard_index)**(-1) * integral\n",
        "print(f\"{distance=}\")\n",
        "print(f\"\\t{Jaccard_index=}\")\n",
        "print(f\"\\t\\t{card_inter=}\", f\"{card_union=}\")\n",
        "print(f\"\\t\\t\\t{c_A=}\")\n",
        "print(f\"\\t\\t\\t{c_B=}\")\n",
        "print(f\"\\t{integral=}\")\n",
        "print(f\"\\t\\t{value=}\")\n",
        "print(f\"\\t\\t\\t{p_A=}\")\n",
        "print(f\"\\t\\t\\t{p_B=}\")\n",
        "print()\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
